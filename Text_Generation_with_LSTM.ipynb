{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6c6b3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Embedding\n",
    "from pickle import dump,load\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9f9419",
   "metadata": {},
   "source": [
    "### Project Name: Building an LSTM Text Generation Model\n",
    "\n",
    "#### Description\n",
    "In this project, I'll use the book **\"Alice in Wonderland\"** to train a text generation model based on **LSTM (Long Short-Term Memory)**.  \n",
    "The model predicts the next word based on a sequence of previous words. Sequences from the book, will form our training data.\n",
    "\n",
    "---\n",
    "\n",
    "#### Table of Contents\n",
    "\n",
    "##### Part 1: Preprocessing\n",
    "1. Read the text file of *Alice in Wonderland*.  \n",
    "2. Use **spaCy** to split the text into words and punctuation, and remove the punctuation.  \n",
    "3. Divide the text into sequences.  \n",
    "4. Use **Keras' Tokenizer** to assign each token an index based on its frequency in the text.  \n",
    "5. Create the training data by splitting the entire text into sequences and a predicted word.\n",
    "\n",
    "##### Part 2: Building and Training the Model\n",
    "6. Build the **LSTM model**.  \n",
    "7. Train the model using the sequences prepared in Part 1.\n",
    "\n",
    "##### Part 3: Text Generation\n",
    "8. Write a function that predicts the next word given a sequence, using the trained model.  \n",
    "9. Generate new text using the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21ab832",
   "metadata": {},
   "source": [
    "### ðŸ“š Data\n",
    "\n",
    "The text of **Alice in Wonderland** was used from the **Gutenberg Project**.  \n",
    "It can be found and downloaded at: [https://www.gutenberg.org/ebooks/11](https://www.gutenberg.org/ebooks/11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0940b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbfef81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read the txt file of Alice in Wonderland\n",
    "\n",
    "def read_file(filepath):\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        str_text = f.read()\n",
    "    return str_text\n",
    "\n",
    "Alice = read_file('Alice_in_Wonderland.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9301524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. We use spacy to split the text into words and punctuations, and remove the punctuation \n",
    "# we load annotations from en_core_web_md, if you do not have it yet, run in terminal:\n",
    "# python -m spacy download en_core_web_md\n",
    "\n",
    "\n",
    "# Load the medium English model, disabling unnecessary components\n",
    "nlp = spacy.load('en_core_web_md', disable=['parser', 'tagger', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85fae827",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_length = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b735b7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we separate punctuations and new lines from the actual text using the following function\n",
    "\n",
    "def separate_punc(doc_text):\n",
    "    return [token.text.lower() for token in nlp(doc_text) if token.text not in \n",
    "           '\\n    \\n \\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29b4a3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = separate_punc(Alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72e604e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33034"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f7e3244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. We devide the text into sequences of 25 words, our network predicts #26\n",
    "\n",
    "train_len = 25+1\n",
    "text_sequences = []\n",
    "for i in range(train_len, len(tokens)):\n",
    "    seq = tokens[(i- train_len):i]\n",
    "    text_sequences.append(seq)\n",
    "\n",
    "text_sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a269b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. We use keras's tokenizer to give each token we have an index based on its frequency in the tex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "422dadd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abb1e671",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d2a3dc97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3227"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size = len(tokenizer.word_counts)\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9290d538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3227,   53,   54, ...,    4,  306,   85],\n",
       "       [  53,   54,  359, ...,  306,   85,  934],\n",
       "       [  54,  359,    7, ...,   85,  934,    7],\n",
       "       ...,\n",
       "       [ 285,    5,    1, ...,    5,  287,   46],\n",
       "       [   5,    1,   53, ...,  287,   46,  587],\n",
       "       [   5,    1,   53, ...,  287,   46,  587]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Using our Tokenization we create the training data:\n",
    "# splitting the entire text into sequences and a predicted word\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "adae074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sequences[:,:-1] # taking the sequences without the predicted word (in the last col): X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16214883",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sequences[:,-1] # the predicted word (label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4674c2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We turn the to-be-predicted word in each sequence and convert it to one-hot encoding\n",
    "# This is for the soft max function in the end of the model \n",
    "y = to_categorical(y,num_classes=vocabulay_size+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ef28db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e545cd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Building and Training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3c9a5559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. We build our LSTM Model\n",
    "\n",
    "def create_model(vocabulary_size,seq_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_size,seq_len,input_length = seq_len))\n",
    "    model.add(LSTM(seq_len*6,return_sequences = True))\n",
    "    model.add(LSTM(seq_len*6))\n",
    "    model.add(Dense(seq_len*6,activation = 'relu'))\n",
    "    \n",
    "    model.add(Dense(vocabulary_size,activation = 'softmax'))\n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy',optimizer = 'adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4aec6ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 25, 25)            80700     \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 25, 150)           105600    \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 150)               180600    \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 150)               22650     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 3228)              487428    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 876,978\n",
      "Trainable params: 876,978\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocabulary_size+1,seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "316e8a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. We use the sequences we prepared to train it \n",
    "# Since Traning with the parameters bellow took ~3 hours, I have saved my trained model\n",
    "# and you can just load it\n",
    "#model.fit(X,y,batch_size = 128, epochs = 200,verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "15024848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('My_Alice_LSTM_Model.h5')\n",
    "#dump(tokenizer,open('My_Alice_Tokenizer','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3d09b56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('My_Alice_LSTM_Model.h5')\n",
    "tokenizer = load(open('My_Alice_Tokenizer','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531e7f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3: Text Generation\n",
    "# 8. We write a function that predict the next word given a sequence, using our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "965c750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model,tokenizer,seq_len,seed_text,num_gen_words):\n",
    "    output_text = []\n",
    "    input_text = seed_text\n",
    "    for i in range(num_gen_words):\n",
    "        # convert the seed to tokens \n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0] \n",
    "        # padding or cutting the sequence to be 25 tokens\n",
    "        pad_encoded = pad_sequences([encoded_text],maxlen = seq_len,truncating='pre')\n",
    "        #predict the next word\n",
    "        pred = model.predict(pad_encoded, verbose=0)\n",
    "        pred_word_ind = np.argmax(pred, axis=-1)[0]\n",
    "        pred_word = tokenizer.index_word[pred_word_ind]\n",
    "        # we update the input text since we predict more than one word\n",
    "        input_text += ' '+pred_word\n",
    "        # adding the predicted words to the output variable \n",
    "        output_text.append(pred_word)\n",
    "    return ' '.join(output_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f2cd3097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. We generate text \n",
    "# here we choose a random sequence for the book to be our input\n",
    "random.seed(55)\n",
    "random_pick = random.randint(0,len(text_sequences))\n",
    "random_seed_text = text_sequences[random_pick]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f39a007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = ' '.join(random_seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "43491512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thimble looking as solemn as she could the next thing was to show the neck she was getting a advantage in it busily painting all'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and here we generate the text\n",
    "generate_text(model,tokenizer,seq_len,seed_text= seed_text, num_gen_words=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
